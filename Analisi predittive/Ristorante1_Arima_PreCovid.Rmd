---
title: "ARIMA - Ristorante1"
output:
  prettydoc::html_pretty:
    df_print: paged
    highlight: vignette
    theme: architect
    toc: yes
    toc_depth: 5
  beamer_presentation:
    colortheme: lily
    fig_caption: no
    fig_height: 6
    fig_width: 7
    fonttheme: structurebold
    theme: Hannover
    toc: yes
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '5'
  pdf_document:
    toc: yes
    toc_depth: 5
  slidy_presentation:
    highlight: default
  ioslides_presentation:
    css:
    - css/fonts.css
    - css/custom.css
    - css/title-slide.css
    - css/slide-background.css
    includes:
      before_body: html/TimeSeriesAnalysis.html
    toc: yes
    transition: default
    widescreen: yes
course: Progetto Data Science Lab
---

```{r}
# Clean Workspace
rm(list=ls())
```

```{r setup, include=FALSE}
# Use 'verbatim = TRUE' as chunk option to show chunk code as is
require(knitr)
hook_source_def = knit_hooks$get('source')
knit_hooks$set(source = function(x, options){
  if (!is.null(options$verbatim) && options$verbatim){
    opts = gsub(",\\s*verbatim\\s*=\\s*TRUE\\s*", "", options$params.src)
    bef = sprintf('\n\n    ```{r %s}\n', opts, "\n")
    stringr::str_c(bef, paste(knitr:::indent_block(x, "    "), collapse = '\n'), "\n    ```\n")
  } else {
     hook_source_def(x, options)
  }
})
```

# Setting & Function

```{r include=FALSE}
set.seed(100)

# Setting librerie utili
# Package names
packages <- c("readxl",  "readr", "forecast", "dplyr", "ggplot2",
              "lubridate", "KFAS", "tseries", "xts", "fastDummies", "TSstudio") 

# Install packages if not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))


# Setting working directory
# working_dir = "C:/Users/marco/OneDrive/UNIMIB_DataScience/99-PROJECTS/DataScienceLab2022/Dati ristoranti"
# setwd(working_dir)

# MAPE 
mape <- function(actual,pred){
  mape <- mean(abs((actual - pred)/actual))*100
  return (mape)
}

#MAE
mae <- function(actual,pred){
  mae <- mean(abs((actual - pred)))
  return (mae)
}
#MSE
rmse <- function(actual, pred){
  rmse <- sqrt(mean((actual - pred)^2))
  return (rmse)
}

# Significatività dei parametri
pars_test <- function(coef, var_coef){
  test <- (1-pnorm(abs(coef)/sqrt(diag(var_coef))))*2
  return(test)
}

# Grafico errore percentuale 
err_plot <- function(actual, pred){
  err_perc <- ((actual - pred)/actual)*100
  return(plot(err_perc, ylab="% errore", main="Errore percentuale di previsione"))

}
```

# Load data and pre-processing

```{r}
df<- read.csv("..//Dati ristoranti//pre-covid_r1.csv", header = TRUE)
```

```{r}
head(df)
```

```{r}
# Trasformo in serie storica
vendite_r1 <- df[,9]
vendite_r1 <- msts(vendite_r1, seasonal.periods=c(7,365.25), ts.frequency = 7) 
# Uso la funzione msts per tenere conto di stagionalità annuale e stagionalità settimanale, impostando però la frequenza della serie storica a 7
head(vendite_r1)
```

# Data exploration

```{r}
plot(vendite_r1)
```

I dati a disposizione sono incassi giornalieri per il Ristorante1. Per dati giornalieri ci possono essere più tipologie di stagionalità. Nel nostro caso abbiamo, molto probabilmente:

-   Stagionalità settimanale

-   Stagionalità annuale

Per quanto riguarda la stagionalità settimanale, grazie alle variabili a disposizione, possiamo subito verificare se effettivamente è presente nei nostri dati.

```{r}
# Media dei giorni feriali

feriali <- df[df$Weekend == 'False' | df$Festivo == FALSE ,c( "lordototale", "Giorno")]
feriali <- tapply(feriali$lordototale, feriali$Giorno, mean, simplify = FALSE)

# Media dei giorni "weekend"
weekend <- df[df$Weekend == 'True',c("lordototale", "Giorno")]
weekend <- tapply(weekend$lordototale, weekend$Giorno, mean, simplify = FALSE)

data.frame(c(feriali,weekend))
```

Notiamo infatti come le medie giornalieri siano sostanzialmente simili dal Lunedi al Giovedì, mentre aumentano notevolmente nei giorni Venerdì-Sabato-Domenica.

Per valutare la presenza di stagionalità annuale, sfruttiamo la variabile "Festivo", valorizzata a True quando siamo in presenza di un Festivo, che non sia però un giorno del weekend.

```{r}
festivo_noweekend <- df[df$Weekend == 'False',c("data", "lordototale", "Giorno", "Festivo")]
tapply(festivo_noweekend$lordototale, festivo_noweekend$Festivo, mean)
```

Le medie nei giorni festivi risultano essere maggiori rispetto a quelle dei giorni feriali. Ciò quindi ci porta ad affermare la presenza di stagionalità annuale, anche se difficilmente stimabile per via della quantità ridotta di osservazioni.

# Componenti della serie storica

Andiamo a decomporre la serie storica nelle sue componenti, ovvero: Trend, Stagionalità, eventuale Ciclo

```{r}
# Usiamo la funzione MSTL che ammette eventuali stagionalità multiple
vendite_r1_dec <- mstl(vendite_r1) 

autoplot(vendite_r1_dec)
```

Come precedentemente accennato la stagionalità a 365 giorni non viene stimata per insufficienza di dati (servono almeno due periodi completi). Notiamo la presenza di un trend irregolare, in quanto include la stagionalità annuale non stimata, ma che comunque sembra crescente se non nell'ultima parte della serie storica, dove probabilmente i dati sono già influenzati dall'avvento della pandemia. La stagionalità settimanle ci indica come nella parte centrale della serie storica i picchi settimanali sono stati meno marcati rispetto al resto della serie storica.

```{r}
plot(vendite_r1, main="Trend Vendite giornaliere", xlab ="Time", ylab="Fatturato")
lines(vendite_r1_dec[,2], col="blue", lwd=2)
```

Notiamo come effettivamente le irregolarità del trend sono dovute a tutte le festività non modellate dalla stagionalità settimanale

## Stazionarietà della serie storica

La serie storica è ovviamente non stazionaria, per i seguenti motivi:

-   Presenza di stagionalità (non stazionarietà stagionale)
-   Presenza di un trend (osservazioni sistematicamente sopra o sotto la media)
-   Possibile non stazionarietà in varianza (se guardiamo il grafico della decomposizione, la parte stagionale tende ad un certo punto a restringersi)

```{r}
ndiffs(vendite_r1)
nsdiffs(vendite_r1)
```

```{r}
nsdiffs(diff(vendite_r1))
```


```{r}
kpss.test(vendite_r1)
```


```{r}
vendite_r1_diff7 <- diff(vendite_r1,7)
autoplot(vendite_r1_diff7)
tsdisplay(vendite_r1_diff7)
```

```{r}
ndiffs(vendite_r1_diff7)
```

Non sono ora necessarie altre differenze.

Andiamo a valutare se la non stazionarietà è risolvibile attraverso una trasformazione di BoxCox dei dati

```{r}
BoxCox.lambda(vendite_r1)
```

La stima del lambda ottimale è vicina allo zero (trasformazione log). Valutiamo quindi come cambiano le considerazioni di prima applicando una trasformazione logaritmica ai dati

```{r}
vendite_r1_log <- log(vendite_r1)
autoplot(vendite_r1_log)
```

```{r}
ndiffs(vendite_r1_log)
nsdiffs(vendite_r1_log)
```

Il test ci indica la necessità di una differenziazione, sia stagionale che non.  Possiamo comunque passare il parametro lambda = "auto" ai sucessivi modelli qualora volessimo utilizzare i dati trasformati


```{r}
tsdisplay(vendite_r1_diff7)
```

ACF non decade a zero lentamente e non presenta pattern stagionali.

### Test

```{r}
#  Ljung-Box test
# a non-stationary signal will have a low p-value
lag.length = 25
Box.test(vendite_r1_diff7, lag=lag.length, type="Ljung-Box") # test stationary signal
```

```{r}
library(tseries)
# a series with a trend line will have a unit root and result in a large p-value
adf.test(vendite_r1_diff7,alternative = "stationary")
```

```{r}
# Kwiatkowski-Phillips-Schmidt-Shin test
# a low p-value will indicate a signal that is not trend stationary, has a unit root
kpss.test(vendite_r1_diff7, null="Trend")
```

Il test di Ljung-Box valuta se sono presenti correlazioni significative ai vari ritardi (essendo che una serie storica stazionaria non dovrebbe avere correlazioni significative ai vari lag). Il test, come già notavamo dall'ACF e PACF ci indica che la serie storica non è stazionaria. Questo è dovuto al fatto che non stiamo considerando una stagionalità annuale, la quale viene considerata come una componente che rende la seire storica non stazionaria.

Gli altri test, non ci indicano la presenza di una non stazionarietà della seire storica.

Procediamo a stimare i vari modelli, cercando di catturare i movimenti stagionali annuali attraverso regressori o altri metodi.

## Variabili stagionali

```{r}
# Dummy settimanali
require('fastDummies')
dum_day_df <- df[, c("data", "Giorno")]
dum_day_df[,"Giorno"] <- as.factor(dum_day_df$Giorno)
dum_day_df <- dummy_cols(dum_day_df, select_columns = c("Giorno"), 
                      # remove_first_dummy = TRUE, 
                      remove_selected_columns = TRUE)
dum_day_df <- subset(dum_day_df, select = -c(data,Giorno_Monday))
dum_day_df
```
```{r}
# La funzione Arima vuole in input una matrice per i regressori
dum_day <- as.matrix(dum_day_df)
```



```{r}
# trasformo in ts 
# train_dumday <- ts(train_dumday, frequency = 7, start = 1, end= 75)
# test_dumday <- ts(test_dumday, frequency = 7, start = c(75,2), end= c(77,7))
```


```{r}
#dummy annuali
dum_ann_df <- read_xlsx("..\\Dati aggiuntivi\\fest_precovid.xlsx", col_types = NULL)
dum_ann_df <- subset(dum_ann_df, select = -date)

dum_ann_df <- as.data.frame(lapply(dum_ann_df, as.integer)) # Trasformo tutte le colonne in interi e ri-trasformo in dataframe
head(dum_ann_df)
```

```{r}
dum_ann_df <- cbind(dum_ann_df, dum_day_df, df$GASOLIO_RISCALDAMENTO.LITRO, df$Pioggia) # Aggiungo due regressori considerati utili

colnames(dum_ann_df)[24] = "Riscaldamento" # Rinonimo
colnames(dum_ann_df)[25] = "Pioggia_True" # Rinonimo

dum_ann_df$Pioggia_True[dum_ann_df$Pioggia_True == "True"] <- 1 # Trasformo in dummy
dum_ann_df$Pioggia_True[dum_ann_df$Pioggia_True == ""] <- 0
dum_ann_df$Pioggia_True <- as.integer(dum_ann_df$Pioggia_True) # Trasformo in integer

head(dum_ann_df)
```

```{r}
dum_ann <- as.matrix(dum_ann_df) # Funzione Arima vuole una matrice in input per i regressori
```



```{r}
# trasformo in ts 
# train_dumann <- ts(train_dumann, frequency = 7, start = 1, end= 75)
# test_dumann <- ts(test_dumann, frequency = 7, start = c(75,2), end= c(77,7))
```

```{r}
# Fourier terms
# 3 sinusoidi per la stagionalità settimanale
# 15 sinusoidi per la stagionalità annuale
four <- fourier(vendite_r1, K=c(3,15))
four <- as.matrix((four))
```



```{r}
# trasformo in ts 
# train_four <- ts(train_four, frequency = 7, start = 1, end= 75)
# test_four <- ts(test_four, frequency = 7, start = c(75,2), end= c(77,7))
```

# Stima modelli

## Partizionamento dei datasets


```{r}
split_vendite <- ts_split(vendite_r1, sample.out =round(length(vendite_r1)*0.2))
train <- split_vendite$train
test <- split_vendite$test
length(train)
length(test)
```

```{r}
# Divisione training-test matrice dummy giornaliere

train_date <- nrow(dum_day) *0.8
train_dumday<- dum_day[1:train_date,]
test_dumday <- dum_day[-c(1:train_date),]

# Controllo le dimensioni di training e test
length(train_dumday[,1])
length(test_dumday[,1])
```

```{r}
# Divisione training-test dummy annuali

train_date <- nrow(dum_ann) *0.8
train_dumann<- dum_ann[1:train_date,]
test_dumann <- dum_ann[-c(1:train_date),]

# Controllo le dimensioni di training e test
length(train_dumday[,1])
length(test_dumday[,1])
```

```{r}
# Divisione training-test termini di Fourier

train_date <- nrow(four) *0.8
train_four<- four[1:train_date,]
test_four <- four[-c(1:train_date),]

# Controllo le dimensioni di training e test
length(train_four[,1])
length(test_four[,1])
```

```{r}
tsdisplay(diff(train, 7))
```


## Modello 1

-   Differenza stagionale di ordine 1

-   Componente AR stagionale di ordine 1:

-   Componente MA stagionale di ordine 1:

-   Componente MA non stagionale di ordine 2

-   Componente AR non stagionale di ordine 3

-   Costante inclusa

```{r}
mod1 <- Arima(y = train,
              order = c(3, 0, 2),
              list(order = c(1, 1, 2)),
              include.drift = FALSE,
              lambda = "auto"
              )
```

### Diagnostica:

```{r}
summary(mod1)
```

```{r}
tsdisplay(mod1$residuals) 
```

```{r}
checkresiduals(mod1, test = FALSE)
```

```{r}
checkresiduals(mod1, test = "LB", lag = 25, plot = FALSE)
```

```{r}
pars_test(mod1$coef, mod1$var.coef)
```
```{r}
mod1_auto <- auto.arima(train, lambda = "auto", stepwise = FALSE, approximation = FALSE) # Minimizza AIC

```

```{r}
summary(mod1_auto)
```

```{r}
tsdisplay(mod1_auto$residuals) 
```

```{r}
checkresiduals(mod1_auto, test = FALSE)
```

```{r}
checkresiduals(mod1_auto, test = "LB", lag = 25, plot = FALSE)
```

```{r}
pars_test(mod1_auto$coef, mod1_auto$var.coef)
```
```{r}
# Fit prime 10 settimane
plot(train[1:70], 
     col = "blue", lwd=0.5, 
     main = "Fitted Values", type = "l")
lines(mod1$fitted[1:70], col="red", lwd=3)
```

```{r}
# Fit prime 10 settimane
plot(train[1:70], 
     col = "blue", lwd=0.5, 
     main = "Fitted Values", type = "l")
lines(mod1_auto$fitted[1:70], col="red", lwd=3)
```

I due modelli presentano caratteristische molto simili, in termini di erorre di previsione (RMSE, MAE, MAPE) sia in termini di AIC. Il modello *mod1_auto* risulta però avere un numero inferiore di paramtri. Si decide quindi di considerare quello.

### Previsioni

```{r}
# Al momento vengono fatte previsioni 44 passi in avanti (lunghezza del validation set)
pred_mod1 <- forecast(mod1_auto, h = length(test), 
                      level = 95)
```

```{r}
autoplot(pred_mod1)
```

```{r}
plot(test, 
     col = "blue", lwd=0.5, 
     ylim = c(min(pred_mod1$lower), max(pred_mod1$upper)), main = "Predictions")
lines(pred_mod1$mean, col="red", lwd=3)

```

```{r}
mape(test, pred_mod1$mean)
```

```{r}
mae(test, pred_mod1$mean)
```

```{r}
rmse(test, pred_mod1$mean)
```

```{r}
err_plot(test, pred_mod1$mean)
```

## Modello 2

Includiamo le dummy giornaliere

```{r}
mod2 <- Arima(y = train,
              order = c(3, 1, 1),
              list(order = c(1, 0, 1)),
              include.constant = TRUE,
              xreg = train_dumday,
              lambda = "auto"
              )
```

```{r}
summary(mod2)
```
```{r}
tsdisplay(mod2$residuals) 
```

```{r}
checkresiduals(mod2, test = FALSE)
```

```{r}
checkresiduals(mod2, test = "LB", lag = 25, plot = FALSE)
```

```{r}
pars_test(mod2$coef, mod2$var.coef)
```

```{r}
mod2_auto <- auto.arima(train, xreg = train_dumday, lambda = "auto", stepwise = FALSE, approximation = FALSE)
```

```{r}
summary(mod2_auto)
```
```{r}
tsdisplay(mod2_auto$residuals) 
```

```{r}
checkresiduals(mod2_auto, test = FALSE)
```

```{r}
checkresiduals(mod2_auto, test = "LB", lag = 25, plot = FALSE)
```

```{r}
pars_test(mod2_auto$coef, mod2_auto$var.coef)
```

```{r}
# Fit prime 10 settimane
plot(train[1:70], 
     col = "blue", lwd=0.5, 
     main = "Fitted Values", type = "l")
lines(mod2_auto$fitted[1:70], col="red", lwd=3)
```


Anche in questo caso i modelli presentano caratteristiche molto simile. Scegliamo il modello *mod2_auto* in quanto presenta un numero di parametri inferiore

### Previsioni 


```{r}
pred_mod2 <- forecast(mod2_auto, h = length(test), 
                      level = 95,
                      xreg = test_dumday)
```

```{r}
autoplot(pred_mod2)
```

```{r}
plot(test, 
     col = "blue", lwd=0.5, 
     ylim = c(min(pred_mod2$lower), max(pred_mod2$upper)), main = "Predictions")
lines(pred_mod2$mean, col="red", lwd=3)
```

```{r}
mape(test, pred_mod2$mean)
```

```{r}
mae(test, pred_mod2$mean)
```

```{r}
rmse(test, pred_mod2$mean)
```

```{r}
err_plot(test, pred_mod1$mean)
```




## Modello 3

Come regressori vengono usati i termini di fourier: 3 armoniche per la stagionalità settimanale e 15 per la stagionalità annuale
```{r}
mod3 <- Arima(y = train,
              order = c(3, 1, 2),
              list(order = c(1, 0, 1)),
              include.constant = TRUE,
              xreg = train_four,
              lambda = "auto"
              )
```



```{r}
summary(mod3)
```

```{r}
tsdisplay(mod3$residuals) 
```

```{r}
checkresiduals(mod3, test = FALSE)
```

```{r}
checkresiduals(mod3, test = "LB", lag = 25, plot = FALSE)
```

```{r}
pars_test(mod3$coef, mod3$var.coef)
```

```{r}
# Fit prime 10 settimane
plot(train[1:70], 
     col = "blue", lwd=0.5, 
     main = "Fitted Values", type = "l")
lines(mod3$fitted[1:70], col="red", lwd=3)
```


```{r}
# Attenzione a runnare, ci mette un po' !
# mod3_auto <- auto.arima(train, xreg = train_four, lambda = "auto", stepwise = FALSE, approximation = FALSE)
```

```{r}
# summary(mod3_auto)
```
```{r}
#tsdisplay(mod3_auto$residuals) 
```

```{r}
# checkresiduals(mod3_auto, test = FALSE)
```

```{r}
# checkresiduals(mod3_auto, test = "LB", lag = 25, plot = FALSE)
```

```{r}
# pars_test(mod3_auto$coef, mod3_auto$var.coef)
```

```{r}
# Fit prime 10 settimane
# plot(train[1:70], 
  #   col = "blue", lwd=0.5, 
   #  main = "Fitted Values", type = "l")
#lines(mod3_auto$fitted[1:70], col="red", lwd=3)
```

Scelgo il modello *mod3* in quanto presenta caratteristiche migliori per quanto riguarda errori di previsioni e residui, anche se risulta essere meno parsimonioso


```{r}
pred_mod3 <- forecast(mod3, h = length(test), 
                      level = 95,
                      xreg = test_four
                      )
```

```{r}
autoplot(pred_mod3)
```

```{r}
plot(test, 
     col = "blue", lwd=0.5, 
     ylim = c(min(pred_mod3$lower), max(pred_mod3$upper)), main = "Predictions")
lines(pred_mod3$mean, col="red", lwd=3)
```

```{r}
mape(test, pred_mod3$mean)
```

```{r}
mae(test, pred_mod3$mean)
```

```{r}
rmse(test, pred_mod3$mean)
```

```{r}
err_plot(test, pred_mod1$mean)
```

## Modello 4

Includiamo le dummy giornaliere e quelle annuali

```{r}
mod4 <- Arima(y = train,
              order = c(3, 1, 3),
              list(order = c(1, 0, 0)),
              include.constant = TRUE,
              xreg = train_dumann,
              lambda = "auto"
              )
```



```{r}
summary(mod4)
```


```{r}
tsdisplay(mod4$residuals) 
```

```{r}
checkresiduals(mod4, test = FALSE)
```

```{r}
checkresiduals(mod4, test = "LB", lag = 25, plot = FALSE)
```

```{r}
pars_test(mod4$coef, mod4$var.coef)
```

```{r}
# Fit prime 10 settimane
plot(train[1:70], 
     col = "blue", lwd=0.5, 
     main = "Fitted Values", type = "l")
lines(mod4$fitted[1:70], col="red", lwd=3)
```

```{r}
# mod4_auto <- auto.arima(train, xreg = train_dumann, lambda = "auto", stepwise = FALSE, approximation = FALSE)
```

```{r}
# summary(mod4_auto)
```


```{r}
# tsdisplay(mod4_auto$residuals) 
```

```{r}
# checkresiduals(mod4_auto, test = FALSE)
```

```{r}
# checkresiduals(mod4_auto, test = "LB", lag = 25, plot = FALSE)
```

```{r}
# pars_test(mod4_auto$coef, mod4_auto$var.coef)
```

Scegliamo il modello *mod4* in quanto presenta caratteristiche migliori in termini di previsioni e residui.


```{r}
pred_mod4 <- forecast(mod4, h = length(test), 
                      level = 95,
                      xreg = test_dumann
                      )
```

```{r}
autoplot(pred_mod4)
```

```{r}
plot(test, 
     col = "blue", lwd=0.5, 
     ylim = c(min(pred_mod4$lower), max(pred_mod4$upper)), main = "Predictions")
lines(pred_mod4$mean, col="red", lwd=3)
```

```{r}
mape(test, pred_mod4$mean)
```

```{r}
mae(test, pred_mod4$mean)
```

```{r}
rmse(test, pred_mod4$mean)
```

```{r}
err_plot(test, pred_mod4$mean)
```


# Confronto modelli

```{r}
train_eval <- data.frame(Modelllo=c("Modello1", "Modello2", "Modello3", "Modello4"),
                         RMSE = c(rmse(train, mod1_auto$fitted), 
                                rmse(train, mod2_auto$fitted), 
                                rmse(train, mod3$fitted),
                                rmse(train, mod4$fitted)),
                         MAE = c(mae(train, mod1_auto$fitted),
                                mae(train, mod2_auto$fitted),
                                mae(train, mod3$fitted),
                                mae(train, mod4$fitted)),
                         MAPE = c(mape(train, mod1_auto$fitted),
                                  mape(train, mod2_auto$fitted),
                                  mape(train, mod3$fitted),
                                  mape(train, mod4$fitted)),
                         AIC = c(mod1_auto$aic, mod2_auto$aic, mod3$aic, mod4$aic))
```


```{r}
test_eval <- data.frame(Modelllo=c("Modello1", "Modello2", "Modello3", "Modello4"),
                         RMSE = c(rmse(test, pred_mod1$mean), 
                                rmse(test, pred_mod2$mean), 
                                rmse(test, pred_mod3$mean),
                                rmse(test, pred_mod4$mean)),
                         MAE = c(mae(test, pred_mod1$mean),
                                mae(test, pred_mod1$mean),
                                mae(test, pred_mod3$mean),
                                mae(test, pred_mod4$mean)),
                         MAPE = c(mape(test, pred_mod1$mean),
                                  mape(test, pred_mod2$mean),
                                  mape(test, pred_mod3$mean),
                                  mape(test, pred_mod4$mean)))

```

```{r}
train_eval
test_eval
```


# Evaluation: Confronto forecasting performance dei modelli

Prima si eseguire questa parte del notebook eseguire le celle sopra.

## Scelta misura di *forecasting accuracy*

Al fine di confrontare i diversi modelli definiti precedentemente è importante valutare le performance in termini forecasting accuracy. Prima di definire la procedura di calcolo della forecasting accuracy è importante selezionare la propria misura di accuratezza.

Si sceglie di utilizzare due misure di accuratezza (**MAE, RMSE e MSE**) di tipo **scale-dependent** perchè:

-   Misure di accuratezza basate su percentage errors e scaled errors (unit-free) sono indicate per comparare le performance di forecasting tra data set diversi

-   Errori di tipo scale-dependent sono nella stessa scala dei dati, ma in questo caso non è un problema

-   Si evitano tutte le problematiche relative alle misure di percentage errors legate:

    -   al fatto che l'errore viene diviso per il valore dell'osservazione (valori infiniti per y che tende a 0)

    -   penalità maggiori per errori negativi piuttosto che quelli positivi

## Procedura di *Cross-Validation* con *rolling forecasting origin*

Effettuo la procedura di *Time-Series Cross Validation* o *Evaluating on a Rolling Forecasting Origin* al fine di:

-   Confrontare i diversi modelli in termini di Performance di Forecasting

-   Potera allenare i modelli sull'intero dataset e non perdere i dati del test set (essendo questo già piccolo)

-   Ottenere comunque due misure di performarmance di accuratezza di forecasting accuracy

-   Determino la forecasting accuracy (come MAE/RMSE/MSE) per k-step avanti dove k (chiamato h, orizzonte, nella funzione `tsCV()`) sarà il mio orizzonte di previsione nel periodo covid

    -   Confrontare l'andamento dei diversi modelli all'aumentare degli step ci consentirà di selezionare

-   Parametri di cross-validation -\> Scelgo come tipologia di Time Series Cross Validation quella **Constant Holdout**

    -   Scelgo arbitrariamente una finestra iniziale (**fixed origin**) di training (il numero minimo di osservazioni necessario a stimare il modello) come 60 -\> parametro `initial`

    -   **Non-Constant Holdout** -\> in modo da utilizzare tutti i dati per il training (altrimenti il training si fermerebbe all'osservazione n-h)

    -   **Non-Constant In-Sample -\>** Non impostiamo il parametro `window`, che altrimenti andrebbe a settare un dimensione fissata del training set e quindi una moving window


## Definizione funzione di Time Series Cross-Validation (`tsCV_UCM()`)

```{r}
source("My-TSCrossValidation-Functions.R")
```


## Definizione funzioni dei modelli

```{r}

# Calcolo tempo di computazione
start.time <- Sys.time()
print(start.time)

# Definisco la forecast-function per i diversi modelli

#1_ARIMA
f_mod1 <- function(x, h, xreg, newxreg) { 
  forecast(Arima(x,
                 order = c(1, 0, 0),
                 seasonal = list(order = c(0, 1, 2)),
                 include.constant = FALSE,
                 lambda = "auto"),
           h = h)
}

#2_ARIMA
f_mod2 <- function(x, h, xreg, newxreg) { 
  forecast(Arima(x,
                 order = c(2, 1, 3),
                 xreg = xreg,
                 include.constant = TRUE,
                 lambda = "auto"),
           h = h,
           xreg=newxreg)
}

#3_ARIMA
f_mod3 <- function(x, h, xreg, newxreg) { 
  forecast(Arima(x,
                 order = c(3, 1, 2),
                 seasonal = list(order = c(1, 0, 1)),
                 xreg = xreg,
                 include.constant = TRUE,
                 lambda = "auto"),
           h = h,
           xreg=newxreg)
}
#4_SARIMAX
f_mod4 <- function(x, h, xreg, newxreg) { 
  forecast(Arima(x,
                 order = c(3, 1, 3),
                 seasonal = list(order = c(1, 0, 0)),
                 xreg = xreg,
                 include.constant = TRUE,
                 lambda = "auto"),
           h = h,
           xreg=newxreg)
}

```
## Cross-Validation e eventuali processing ad hoc

```{r}
# Parametri di cross-validation globali per i primi due modelli
h = 42 # 6 settimane
initial = 126 # 3 volte l'orizzonte
window = NULL

start.time <- Sys.time()
print(start.time)

# Stima degli errori di previsione con CV

e_1 <- tsCV_ARIMA(y = vendite_r1, forecastfunction = f_mod1, h=h, initial = initial, window = window)
#e1 <- tail(e1, n = -initial)
e1 <- e_1$e
e1_percentage <- e_1$e_percentage
e1_estimate <- e_1$y_estimate
e1_groundtruth <- e_1$y_groundtruth


xreg2 <- dum_day
e_2 <- tsCV_ARIMA(y = vendite_r1, forecastfunction = f_mod2, h=h, xreg=xreg2, initial = initial)
#e2 <- tail(e1, n = -initial)
e2 <- e_2$e
e2_percentage <- e_2$e_percentage
e2_estimate <- e_2$y_estimate
e2_groundtruth <- e_2$y_groundtruth


end.time <- Sys.time()
print(end.time)
time.taken <- end.time - start.time
print(time.taken)

```

```{r}
# Parametri di cross-validation globali per il terzo e quarto modello

h = 42 # 6 settimane
initial = 365 #126*3, modelli maggiormente complessi richiedono un training set iniziale più ampio
window = NULL

start.time <- Sys.time()
print(start.time)


xreg3 <- four
e_3 <- tsCV_ARIMA(y = vendite_r1, forecastfunction = f_mod3, h=h, xreg=xreg3, initial = initial)
#e3 <- tail(e3, n = -initial)
e3 <- e_3$e
e3_percentage <- e_3$e_percentage
e3_estimate <- e_3$y_estimate
e3_groundtruth <- e_3$y_groundtruth

xreg4 <- dum_ann
e_4 <- tsCV_ARIMA(y = vendite_r1, forecastfunction = f_mod4, h=h, xreg=xreg4, initial = initial)
#e4 <- tail(e4, n = -initial)
e4 <- e_4$e
e4_percentage <- e_4$e_percentage
e4_estimate <- e_4$y_estimate
e4_groundtruth <- e_4$y_groundtruth

end.time <- Sys.time()
print(end.time)
time.taken <- end.time - start.time
print(time.taken)
```

## Analisi degli errori

### Errori troncati

Andiamo a tenere solo 1 riga su h della matrice degli errori, simulando quindi un avanzamento di training e test di h osservazioni ogni iterazione.

```{r}
test <- as.data.frame(e1)
indices <- seq(1, nrow(test), by = 1)
test1 <- subset(test, row.names(test) %in% indices)

RMSE_mod1_test <- sqrt(colMeans(test1^2, na.rm = TRUE))

plot(1:42, RMSE_mod1_test, type="l", col=1, xlab="horizon", ylab="RMSE", ylim = c(2500,6000))

```
Vediamo sui modelli con regressori

```{r}
test <- as.data.frame(e3)
indices <- seq(1, nrow(test), by = 7)
test1 <- subset(test, row.names(test) %in% indices)

RMSE_mod3_test <- sqrt(colMeans(test1^2, na.rm = TRUE))

plot(1:42, RMSE_mod3_test, type="l", col=1, xlab="horizon", ylab="RMSE", ylim = c(2500,6000))
```

### Studio specifico degli errori 

#### 1-step

```{r}
check1 <- cbind(e1[,"h=1"], e1_estimate[,"h=1"], e1_groundtruth[,"h=1"], c(df$data[2:(nrow(df))], NA), c(df$lordototale[2:(nrow(df))], NA), c(df$Giorno[2:(nrow(df))], NA))
colnames(check1) <- c("e1", "e1_estimate", "e1_groundtruth", "data", "lordototale", "Giorno")
# tengo solo righe con errori molto elevati
check1 <- as_tibble(check1)
check1[,1] <- as.numeric(unlist(check1[,1]))
check1 <- filter(check1, abs(e1) > 4000)
print(nrow(check1))
View(check1)

check2 <- cbind(e2[,"h=1"], e2_estimate[,"h=1"], e2_groundtruth[,"h=1"], c(df$data[2:(nrow(df))], NA), c(df$lordototale[2:(nrow(df))], NA), c(df$Giorno[2:(nrow(df))], NA))
colnames(check2) <- c("e2", "e2_estimate", "e2_groundtruth", "data", "lordototale", "Giorno")
# tengo solo righe con errori molto elevati
check2 <- as_tibble(check2)
check2[,1] <- as.numeric(unlist(check2[,1]))
check2 <- filter(check2, abs(e2) > 4000)
print(nrow(check2))
View(check2)

check3 <- cbind(e3[,"h=1"], e3_estimate[,"h=1"], e3_groundtruth[,"h=1"], c(df$data[2:(nrow(df))], NA), c(df$lordototale[2:(nrow(df))], NA), c(df$Giorno[2:(nrow(df))], NA))
colnames(check3) <- c("e3", "e3_estimate", "e3_groundtruth", "data", "lordototale", "Giorno")
# tengo solo righe con errori molto elevati
check3 <- as_tibble(check3)
check3[,1] <- as.numeric(unlist(check3[,1]))
check3 <- filter(check3, abs(e3) > 4000)
print(nrow(check3))
View(check3)

check4 <- cbind(e4[,"h=1"], e4_estimate[,"h=1"], e4_groundtruth[,"h=1"], c(df$data[2:(nrow(df))], NA), c(df$lordototale[2:(nrow(df))], NA), c(df$Giorno[2:(nrow(df))], NA))
colnames(check4) <- c("e4", "e4_estimate", "e4_groundtruth", "data", "lordototale", "Giorno")
# tengo solo righe con errori molto elevati
check4 <- as_tibble(check4)
check4[,1] <- as.numeric(unlist(check4[,1]))
check4 <- filter(check4, abs(e4) > 4000)
print(nrow(check4))
View(check4)
```
Vediamo se ci sono giorni della settimana particolarmente sbagliati

```{r}
table(check1$Giorno)
table(check2$Giorno)
table(check3$Giorno)
table(check4$Giorno)
```
Vediamo se c'è un giorno dell'anno che tutti i modelli sbagliano

```{r}
common_values <- Reduce(intersect, list(check1$data, check2$data, check3$data, check4$data))
common_values
```

#### 7-step

```{r}
check1 <- cbind(e1[,"h=20"], e1_estimate[,"h=20"], e1_groundtruth[,"h=20"], c(df$data[2:(nrow(df))], NA), c(df$lordototale[2:(nrow(df))], NA), c(df$Giorno[2:(nrow(df))], NA))
colnames(check1) <- c("e1", "e1_estimate", "e1_groundtruth", "data", "lordototale", "Giorno")
# tengo solo righe con errori molto elevati
check1 <- as_tibble(check1)
check1[,1] <- as.numeric(unlist(check1[,1]))
check1 <- filter(check1, abs(e1) > 4000)
print(nrow(check1))
View(check1)

check2 <- cbind(e2[,"h=20"], e2_estimate[,"h=20"], e2_groundtruth[,"h=20"], c(df$data[2:(nrow(df))], NA), c(df$lordototale[2:(nrow(df))], NA), c(df$Giorno[2:(nrow(df))], NA))
colnames(check2) <- c("e2", "e2_estimate", "e2_groundtruth", "data", "lordototale", "Giorno")
# tengo solo righe con errori molto elevati
check2 <- as_tibble(check2)
check2[,1] <- as.numeric(unlist(check2[,1]))
check2 <- filter(check2, abs(e2) > 4000)
print(nrow(check2))
View(check2)

check3 <- cbind(e3[,"h=20"], e3_estimate[,"h=20"], e3_groundtruth[,"h=20"], c(df$data[2:(nrow(df))], NA), c(df$lordototale[2:(nrow(df))], NA), c(df$Giorno[2:(nrow(df))], NA))
colnames(check3) <- c("e3", "e3_estimate", "e3_groundtruth", "data", "lordototale", "Giorno")
# tengo solo righe con errori molto elevati
check3 <- as_tibble(check3)
check3[,1] <- as.numeric(unlist(check3[,1]))
check3 <- filter(check3, abs(e3) > 4000)
print(nrow(check3))
View(check3)

check4 <- cbind(e4[,"h=20"], e4_estimate[,"h=20"], e4_groundtruth[,"h=20"], c(df$data[2:(nrow(df))], NA), c(df$lordototale[2:(nrow(df))], NA), c(df$Giorno[2:(nrow(df))], NA))
colnames(check4) <- c("e4", "e4_estimate", "e4_groundtruth", "data", "lordototale", "Giorno")
# tengo solo righe con errori molto elevati
check4 <- as_tibble(check4)
check4[,1] <- as.numeric(unlist(check4[,1]))
check4 <- filter(check4, abs(e4) > 4000)
print(nrow(check4))
View(check4)
```
```{r}
table(check1$Giorno)
table(check2$Giorno)
table(check3$Giorno)
table(check4$Giorno)
```
```{r}
common_values <- Reduce(intersect, list(check1$data, check2$data, check3$data, check4$data))
common_values
```
#### 30-step

```{r}
check1 <- cbind(e1[,"h=30"], e1_estimate[,"h=30"], e1_groundtruth[,"h=30"], c(df$data[2:(nrow(df))], NA), c(df$lordototale[2:(nrow(df))], NA), c(df$Giorno[2:(nrow(df))], NA))
colnames(check1) <- c("e1", "e1_estimate", "e1_groundtruth", "data", "lordototale", "Giorno")
# tengo solo righe con errori molto elevati
check1 <- as_tibble(check1)
check1[,1] <- as.numeric(unlist(check1[,1]))
check1 <- filter(check1, abs(e1) > 4000)
print(nrow(check1))
View(check1)

check2 <- cbind(e2[,"h=30"], e2_estimate[,"h=30"], e2_groundtruth[,"h=30"], c(df$data[2:(nrow(df))], NA), c(df$lordototale[2:(nrow(df))], NA), c(df$Giorno[2:(nrow(df))], NA))
colnames(check2) <- c("e2", "e2_estimate", "e2_groundtruth", "data", "lordototale", "Giorno")
# tengo solo righe con errori molto elevati
check2 <- as_tibble(check2)
check2[,1] <- as.numeric(unlist(check2[,1]))
check2 <- filter(check2, abs(e2) > 4000)
print(nrow(check2))
View(check2)

check3 <- cbind(e3[,"h=30"], e3_estimate[,"h=30"], e3_groundtruth[,"h=30"], c(df$data[2:(nrow(df))], NA), c(df$lordototale[2:(nrow(df))], NA), c(df$Giorno[2:(nrow(df))], NA))
colnames(check3) <- c("e3", "e3_estimate", "e3_groundtruth", "data", "lordototale", "Giorno")
# tengo solo righe con errori molto elevati
check3 <- as_tibble(check3)
check3[,1] <- as.numeric(unlist(check3[,1]))
check3 <- filter(check3, abs(e3) > 4000)
print(nrow(check3))
View(check3)

check4 <- cbind(e4[,"h=30"], e4_estimate[,"h=30"], e4_groundtruth[,"h=30"], c(df$data[2:(nrow(df))], NA), c(df$lordototale[2:(nrow(df))], NA), c(df$Giorno[2:(nrow(df))], NA))
colnames(check4) <- c("e4", "e4_estimate", "e4_groundtruth", "data", "lordototale", "Giorno")
# tengo solo righe con errori molto elevati
check4 <- as_tibble(check4)
check4[,1] <- as.numeric(unlist(check4[,1]))
check4 <- filter(check4, abs(e4) > 4000)
print(nrow(check4))
View(check4)
```
Vediamo se ci sono giorni della settimana particolarmente sbagliati

```{r}
table(check1$Giorno)
table(check2$Giorno)
table(check3$Giorno)
table(check4$Giorno)
```
Vediamo se c'è un giorno dell'anno che tutti i modelli sbagliano

```{r}
common_values <- Reduce(intersect, list(check1$data, check2$data, check3$data, check4$data))
common_values
```
## Confrontro tra i modelli

### RMSE

```{r}
RMSE_mod1 <- sqrt(colMeans(e1^2, na.rm = TRUE))
RMSE_mod2 <- sqrt(colMeans(e2^2, na.rm = TRUE))
RMSE_mod3 <- sqrt(colMeans(e3^2, na.rm = TRUE))
RMSE_mod4 <- sqrt(colMeans(e4^2, na.rm = TRUE))
#RMSE_mod6 <- sqrt(colMeans(e6^2, na.rm = TRUE))


# Zoom in
plot(1:42, RMSE_mod1, type="l", col=1, xlab="horizon", ylab="RMSE", ylim = c(2500,6000))
lines(1:42, RMSE_mod2, type="l",col=2)
lines(1:42, RMSE_mod3, type="l",col=3)
lines(1:42, RMSE_mod4, type="l",col=4)
legend("topleft",legend=c("1_ARIMA1","2_ARIMA2","3_ARIMA3","4_ARIMA4"),col=1:4,lty=1)

```

### RMdSE

```{r}
RMSE_mod1 <- sqrt(colMedians(e1^2, na.rm = TRUE, hasNA = TRUE))
RMSE_mod2 <- sqrt(colMedians(e2^2, na.rm = TRUE, hasNA = TRUE))
RMSE_mod3 <- sqrt(colMedians(e3^2, na.rm = TRUE, hasNA = TRUE))
RMSE_mod4 <- sqrt(colMedians(e4^2, na.rm = TRUE, hasNA = TRUE))
#RMSE_mod6 <- sqrt(colMeans(e6^2, na.rm = TRUE))


# Zoom in
plot(1:42, RMSE_mod1, type="l", col=1, xlab="horizon", ylab="RMSE", ylim = c(0,3500))
lines(1:42, RMSE_mod2, type="l",col=2)
lines(1:42, RMSE_mod3, type="l",col=3)
lines(1:42, RMSE_mod4, type="l",col=4)
legend("topleft",legend=c("1_ARIMA1","2_ARIMA2","3_ARIMA3","4_ARIMA4"),col=1:4,lty=1)

```

**RMSE Medi**

```{r}
mean(RMSE_mod1)
mean(RMSE_mod2)
mean(RMSE_mod3)
mean(RMSE_mod4)
```

### MAE

```{r}
MAE_mod1 <- colMeans(abs(e1), na.rm = TRUE)
MAE_mod2 <- colMeans(abs(e2), na.rm = TRUE)
MAE_mod3 <- colMeans(abs(e3), na.rm = TRUE)
MAE_mod4 <- colMeans(abs(e4), na.rm = TRUE)

plot(1:42, MAE_mod1, type="l", col=1, xlab="horizon", ylab="MAE", ylim = c(0,6000))
lines(1:42, MAE_mod2, type="l",col=2)
lines(1:42, MAE_mod3, type="l",col=3)
lines(1:42, MAE_mod4, type="l",col=4)

legend("topleft",legend=c("1_ARIMA1","2_ARIMA2","3_ARIMA3","4_ARIMA4"),col=1:4,lty=1)
```

### MAPE

```{r}
MAPE_mod1 <- colMeans(abs(e1_percentage), na.rm = TRUE)
MAPE_mod2 <- colMeans(abs(e2_percentage), na.rm = TRUE)
MAPE_mod3 <- colMeans(abs(e3_percentage), na.rm = TRUE)
MAPE_mod4 <- colMeans(abs(e4_percentage), na.rm = TRUE)


plot(1:42, MAPE_mod1, type="l", col=1, xlab="horizon", ylab="MAPE", ylim = c(0,30))
lines(1:42, MAPE_mod2, type="l",col=2)
lines(1:42, MAPE_mod3, type="l",col=3)
lines(1:42, MAPE_mod4, type="l",col=4)

legend("topleft",legend=c("1_ARIMA1","2_ARIMA2","3_ARIMA3","4_ARIMA4"),col=1:4,lty=1)
```
